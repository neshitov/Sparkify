{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "### Read data and imports"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import numpy as np\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.sql import types as T\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.sql.functions import lit, udf, struct, countDistinct, collect_list, avg, count, col\nfrom pyspark.sql.types import ArrayType, BooleanType, LongType, FloatType\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.classification import GBTClassifier\n\n# Create spark session\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Sparkify\") \\\n    .getOrCreate()\n\n# Read in full sparkify dataset\nevent_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\ndf = spark.read.json(event_data)", "execution_count": 1, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "20979eab95274a99b293ba49ba5ebd49"}}, "metadata": {}}, {"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1552102017728_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-9-115.us-west-1.compute.internal:20888/proxy/application_1552102017728_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-1-241.us-west-1.compute.internal:8042/node/containerlogs/container_1552102017728_0004_01_000002/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# the list of all pages in the service \npages = df.select('page').distinct().toPandas()['page'].values", "execution_count": 2, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d41a04ab90934576a87e3d71baeade8b"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Feature engineering."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Auxiliary functions to check if user churned\n\ndef churn_state(levels):\n    ''' from a list of user account levels (paid/free) creates a list \n        of booleans indicating if user churned after given moment action'''\n    if len(levels)>1:\n        change = (np.array(levels[1:])!=np.array(levels[:-1]))\n        churned = (change & (np.array(levels[:-1])=='paid'))\n        churned = np.insert(churned, 0, False)\n    else:\n        churned = np.array([False])\n    return churned\n\ndef was_paying(levels):\n    ''' determines if user was on paid level'''\n    return (np.array(levels)=='paid').any().tolist()\n\ndef churned(levels):\n    ''' determines if there was a churn'''\n    return churn_state(levels).any().tolist()\n\ndef num_first_churn(levels):\n    ''' returns the index of the interaction after the first churn event happened'''\n    did_churn = churned(levels)\n    churn_states = churn_state(levels)\n    #assert len(churn_states)>0\n    if not did_churn:\n        num = len(churn_states)-1\n    else:    \n        num = np.argmax(churn_states)\n    return num\n\ndef time_first_churn(levels, timestamps):\n    ''' returns timestamp of the first churn event'''\n    return timestamps[num_first_churn(levels)]", "execution_count": 3, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "967343a47feb48b1a1fb9a663ae5f5d9"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def get_churn_info(log_df):\n    '''\n    creates a dataframe of users indicating if user churned and the timestamp of the first churn event.\n    args:\n        log_df: dataframe of user events\n    returns:\n        churn_info: dataframe with user churn information\n    '''\n    convert = udf(churned, BooleanType())\n    paid = udf(was_paying, BooleanType())\n    _max = udf(lambda x: max(x), LongType())\n    _min = udf(lambda x: min(x), LongType())\n    get_churn_time = udf(lambda x: time_first_churn(x[0],x[1]), LongType())\n    \n    churn_info = log_df.groupby('userId').agg({'level':'collect_list','ts':'collect_list'})\n    churn_info = churn_info.withColumn('last_ts', _max('collect_list(ts)'))\n    churn_info = churn_info.withColumn('first_ts', _min('collect_list(ts)'))\n    churn_info = churn_info.withColumn('churned', convert('collect_list(level)'))\n    churn_info = churn_info.withColumn('was_paying', paid('collect_list(level)'))\n    churn_info = churn_info.withColumn('first_churn_time', get_churn_time(struct('collect_list(level)', 'collect_list(ts)')))\n    churn_info = churn_info.filter(\"was_paying=True\")\n    churn_info = churn_info.select(['userId','churned','first_churn_time','first_ts','last_ts'])\n    return churn_info", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "69e463091cd149ab9aa4f6a545f76e0b"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def user_stats(log_df):\n    '''\n    function that creates features dataset from log dataset\n    args:\n        log_df: dataframe of user events\n    returns:\n        features_df: dataframe with user service usage statistics:\n            'userId': user id\n            'avg_session_duration': average session duration\n            'avg(page)': for every page in the list of service pages, the avergae time of using the page per session\n            'nr_of_sessions': number of sessions\n    '''\n    churn_info = get_churn_info(log_df)\n    relevant_df = log_df.filter(\"userId != ''\")\n    relevant_df = relevant_df.join(churn_info, on='userId')\n    is_before_period = udf(lambda x: (x[0]<=x[1]), BooleanType())\n    \n    relevant_df = relevant_df.withColumn('is_before_period', is_before_period(struct('ts', 'first_churn_time')))\n    relevant_df = relevant_df.filter(\"is_before_period = True\")\n    \n    number_of_sessions = relevant_df.groupby('userId').agg(countDistinct('sessionId')).\\\n                            withColumnRenamed('count(DISTINCT sessionId)', 'nr_of_sessions')\n    _length = udf(lambda x: max(x)-min(x), LongType())\n    avg_session_duration = df.filter(\"userId != ''\").groupby(['userId','sessionId']).agg(collect_list('ts').alias('ts'))\n    avg_session_duration = avg_session_duration.withColumn('duration', _length('ts'))\n    avg_session_duration = avg_session_duration.select(['userId','sessionId','duration'])\n    avg_session_duration = avg_session_duration.groupby('userId').agg(avg('duration').alias('avg_session_duration'))\n    \n    pages_per_session = relevant_df.groupby(['userId','sessionId']).agg(collect_list('page').alias('pages'))\n    for page in pages:\n        count_page = udf(lambda x: x.count(page)/len(x))\n        pages_per_session = pages_per_session.withColumn(page, count_page('pages'))\n    pages_per_session = pages_per_session.drop('pages')\n    expr = {page:'avg' for page in pages}\n    pages_per_session = pages_per_session.groupby('userId').agg(expr)\n    \n    features_df = churn_info.join(pages_per_session, on='userId')\n    features_df = features_df.join(avg_session_duration, on='userId')\n    features_df = features_df.join(number_of_sessions, on='userId')\n    for col in ['was_paying','first_churn_time']:\n        features_df = features_df.drop(col)\n        \n    return features_df", "execution_count": 5, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2f27a0a37f6e4ae4ab32826f732f72eb"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Model training"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Feature scaling"}, {"metadata": {"collapsed": true, "trusted": true}, "cell_type": "code", "source": "user_data = user_stats(df)\nuser_data = user_data.drop('userId')\nfeature_columns = [col for col in user_data.columns if col!='churned']\n\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\nX = assembler.transform(user_data)['features','churned']\nX = X.withColumn('churned_num', X.churned.cast('integer'))['features','churned_num']\n\ntrain, test = X.randomSplit([0.7, 0.3], seed=42)\nscaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\nscaler = scaler.fit(train)\ntrain = scaler.transform(train)['scaled_features','churned_num']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Logistic regression training and evaluating on the test set"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"churned_num\")\nlr = lr.fit(train)\ntest = scaler.transform(test)['scaled_features','churned_num']\npredictions = lr.transform(test)\nevaluator = BinaryClassificationEvaluator(labelCol='churned_num')\nprint('Logistic Regression, test set perfromance, Area Under ROC:', evaluator.evaluate(predictions))", "execution_count": 7, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "18a04185e7fd48fa867d109501bd0cfe"}}, "metadata": {}}, {"output_type": "stream", "text": "('Logistic Regression, test set perfromance, Area Under ROC:', 0.7728633058054973)", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Gradient boosted trees training and evaluating on the test set"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "gbt = GBTClassifier(featuresCol=\"scaled_features\", labelCol=\"churned_num\", maxIter=10)\nmodel = gbt.fit(train)\npredictions_gb = model.transform(test)\nprint('Gradient boosted trees, test set perfromance, Area Under ROC:', evaluator.evaluate(predictions_gb))", "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "55ec525b659840368a1a7c207e4f1a00"}}, "metadata": {}}, {"output_type": "stream", "text": "('Gradient boosted trees, test set perfromance, Area Under ROC:', 0.7949019888338394)", "name": "stdout"}]}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}