{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20979eab95274a99b293ba49ba5ebd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1552102017728_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-9-115.us-west-1.compute.internal:20888/proxy/application_1552102017728_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-1-241.us-west-1.compute.internal:8042/node/containerlogs/container_1552102017728_0004_01_000002/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import lit, udf, struct, countDistinct, collect_list, avg, count, col\n",
    "from pyspark.sql.types import ArrayType, BooleanType, LongType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read in full sparkify dataset\n",
    "event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "df = spark.read.json(event_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41a04ab90934576a87e3d71baeade8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the list of all pages in the service \n",
    "pages = df.select('page').distinct().toPandas()['page'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next several functions are needed to detect customer churn from  a sequnce of imteractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967343a47feb48b1a1fb9a663ae5f5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Auxiliary functions to check if user churned\n",
    "\n",
    "def churn_state(levels):\n",
    "    ''' from a list of user account levels (paid/free) creates a list \n",
    "        of booleans indicating if user churned after given moment action'''\n",
    "    if len(levels)>1:\n",
    "        change = (np.array(levels[1:])!=np.array(levels[:-1]))\n",
    "        churned = (change & (np.array(levels[:-1])=='paid'))\n",
    "        churned = np.insert(churned, 0, False)\n",
    "    else:\n",
    "        churned = np.array([False])\n",
    "    return churned\n",
    "\n",
    "def was_paying(levels):\n",
    "    ''' determines if user was on paid level'''\n",
    "    return (np.array(levels)=='paid').any().tolist()\n",
    "\n",
    "def churned(levels):\n",
    "    ''' determines if there was a churn'''\n",
    "    return churn_state(levels).any().tolist()\n",
    "\n",
    "def num_first_churn(levels):\n",
    "    ''' returns the index of the interaction after the first churn event happened'''\n",
    "    did_churn = churned(levels)\n",
    "    churn_states = churn_state(levels)\n",
    "    #assert len(churn_states)>0\n",
    "    if not did_churn:\n",
    "        num = len(churn_states)-1\n",
    "    else:    \n",
    "        num = np.argmax(churn_states)\n",
    "    return num\n",
    "\n",
    "def time_first_churn(levels, timestamps):\n",
    "    ''' returns timestamp of the first churn event'''\n",
    "    return timestamps[num_first_churn(levels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next function creates a table with churn informations\n",
    "For every user it indicates if the user churned and provides the timestamp of the moment of the churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e463091cd149ab9aa4f6a545f76e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_churn_info(log_df):\n",
    "    '''\n",
    "    creates a dataframe of users indicating if user churned and the timestamp of the first churn event.\n",
    "    args:\n",
    "        log_df: dataframe of user events\n",
    "    returns:\n",
    "        churn_info: dataframe with user churn information\n",
    "    '''\n",
    "    convert = udf(churned, BooleanType())\n",
    "    paid = udf(was_paying, BooleanType())\n",
    "    _max = udf(lambda x: max(x), LongType())\n",
    "    _min = udf(lambda x: min(x), LongType())\n",
    "    get_churn_time = udf(lambda x: time_first_churn(x[0],x[1]), LongType())\n",
    "    \n",
    "    churn_info = log_df.groupby('userId').agg({'level':'collect_list','ts':'collect_list'})\n",
    "    churn_info = churn_info.withColumn('last_ts', _max('collect_list(ts)'))\n",
    "    churn_info = churn_info.withColumn('first_ts', _min('collect_list(ts)'))\n",
    "    churn_info = churn_info.withColumn('churned', convert('collect_list(level)'))\n",
    "    churn_info = churn_info.withColumn('was_paying', paid('collect_list(level)'))\n",
    "    churn_info = churn_info.withColumn('first_churn_time', get_churn_time(struct('collect_list(level)', 'collect_list(ts)')))\n",
    "    churn_info = churn_info.filter(\"was_paying=True\")\n",
    "    churn_info = churn_info.select(['userId','churned','first_churn_time','first_ts','last_ts'])\n",
    "    return churn_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next function creates the feature dataset from user log data\n",
    "For every user the dataset contains binary label indicating if user churned and provides user features:\n",
    "average session duration and fraction of the session time user spends on each of the 22 pages of the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f27a0a37f6e4ae4ab32826f732f72eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def user_stats(log_df):\n",
    "    '''\n",
    "    function that creates features dataset from log dataset\n",
    "    args:\n",
    "        log_df: dataframe of user events\n",
    "    returns:\n",
    "        features_df: dataframe with user service usage statistics:\n",
    "            'userId': user id\n",
    "            'avg_session_duration': average session duration\n",
    "            'avg(page)': for every page in the list of service pages, the avergae time of using the page per session\n",
    "            'nr_of_sessions': number of sessions\n",
    "    '''\n",
    "    churn_info = get_churn_info(log_df)\n",
    "    relevant_df = log_df.filter(\"userId != ''\")\n",
    "    relevant_df = relevant_df.join(churn_info, on='userId')\n",
    "    is_before_period = udf(lambda x: (x[0]<=x[1]), BooleanType())\n",
    "    \n",
    "    relevant_df = relevant_df.withColumn('is_before_period', is_before_period(struct('ts', 'first_churn_time')))\n",
    "    relevant_df = relevant_df.filter(\"is_before_period = True\")\n",
    "    \n",
    "    number_of_sessions = relevant_df.groupby('userId').agg(countDistinct('sessionId')).\\\n",
    "                            withColumnRenamed('count(DISTINCT sessionId)', 'nr_of_sessions')\n",
    "    _length = udf(lambda x: max(x)-min(x), LongType())\n",
    "    avg_session_duration = df.filter(\"userId != ''\").groupby(['userId','sessionId']).agg(collect_list('ts').alias('ts'))\n",
    "    avg_session_duration = avg_session_duration.withColumn('duration', _length('ts'))\n",
    "    avg_session_duration = avg_session_duration.select(['userId','sessionId','duration'])\n",
    "    avg_session_duration = avg_session_duration.groupby('userId').agg(avg('duration').alias('avg_session_duration'))\n",
    "    \n",
    "    pages_per_session = relevant_df.groupby(['userId','sessionId']).agg(collect_list('page').alias('pages'))\n",
    "    for page in pages:\n",
    "        count_page = udf(lambda x: x.count(page)/len(x))\n",
    "        pages_per_session = pages_per_session.withColumn(page, count_page('pages'))\n",
    "    pages_per_session = pages_per_session.drop('pages')\n",
    "    expr = {page:'avg' for page in pages}\n",
    "    pages_per_session = pages_per_session.groupby('userId').agg(expr)\n",
    "    \n",
    "    features_df = churn_info.join(pages_per_session, on='userId')\n",
    "    features_df = features_df.join(avg_session_duration, on='userId')\n",
    "    features_df = features_df.join(number_of_sessions, on='userId')\n",
    "    for col in ['was_paying','first_churn_time']:\n",
    "        features_df = features_df.drop(col)\n",
    "        \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_data = user_stats(df)\n",
    "user_data = user_data.drop('userId')\n",
    "feature_columns = [col for col in user_data.columns if col!='churned']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "X = assembler.transform(user_data)['features','churned']\n",
    "X = X.withColumn('churned_num', X.churned.cast('integer'))['features','churned_num']\n",
    "\n",
    "train, test = X.randomSplit([0.7, 0.3], seed=42)\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler = scaler.fit(train)\n",
    "train = scaler.transform(train)['scaled_features','churned_num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression training and evaluating on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a04185e7fd48fa867d109501bd0cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Logistic Regression, test set perfromance, Area Under ROC:', 0.7728633058054973)"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"churned_num\")\n",
    "lr = lr.fit(train)\n",
    "test = scaler.transform(test)['scaled_features','churned_num']\n",
    "predictions = lr.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='churned_num')\n",
    "print('Logistic Regression, test set perfromance, Area Under ROC:', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve for logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_roc(ax, predictions, labels, title='ROC curve'):\n",
    "    ''' outputs the roc curve on ax axes'''\n",
    "    prediction_prob = predictions.toPandas()['probability'].apply(lambda x:x[1]).values\n",
    "    fpr, tpr, _ = roc_curve(labels, prediction_prob)\n",
    "    ax.plot(fpr, tpr)\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(title)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "show_roc(ax, predictions, labels)\n",
    "plt.save_fig('lr_roc.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient boosted trees training and evaluating on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ec525b659840368a1a7c207e4f1a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gradient boosted trees, test set perfromance, Area Under ROC:', 0.7949019888338394)"
     ]
    }
   ],
   "source": [
    "gbt = GBTClassifier(featuresCol=\"scaled_features\", labelCol=\"churned_num\", maxIter=10)\n",
    "model = gbt.fit(train)\n",
    "predictions_gb = model.transform(test)\n",
    "print('Gradient boosted trees, test set perfromance, Area Under ROC:', evaluator.evaluate(predictions_gb))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "show_roc(ax, predictions_gb, labels)\n",
    "plt.save_fig('gb_roc.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
